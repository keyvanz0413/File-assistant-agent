#!/usr/bin/env python3
"""
æµ‹è¯• Ollama é›†æˆ

è¿™ä¸ªè„šæœ¬å¸®åŠ©ä½ éªŒè¯ Ollama æ˜¯å¦æ­£ç¡®é…ç½®å¹¶å¯ä»¥ä¸ ConnectOnion ä¸€èµ·å·¥ä½œã€‚
"""

import sys
import requests
from connectonion import Agent


def check_ollama_service():
    """æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ"""
    print("ğŸ” æ£€æŸ¥ Ollama æœåŠ¡...")
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=2)
        if response.status_code == 200:
            models = response.json().get("models", [])
            print("âœ… Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ")
            print(f"ğŸ“¦ å·²å®‰è£…çš„æ¨¡å‹: {len(models)} ä¸ª")
            for model in models:
                print(f"   - {model.get('name', 'unknown')}")
            return True, models
        else:
            print("âŒ Ollama æœåŠ¡å“åº”å¼‚å¸¸")
            return False, []
    except requests.exceptions.ConnectionError:
        print("âŒ æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡")
        print("ğŸ’¡ è¯·è¿è¡Œ: ollama serve")
        return False, []
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return False, []


def test_basic_tool(model_name: str):
    """æµ‹è¯•åŸºæœ¬å·¥å…·è°ƒç”¨"""
    print(f"\nğŸ§ª æµ‹è¯•åŸºæœ¬å·¥å…·è°ƒç”¨ (ä½¿ç”¨æ¨¡å‹: {model_name})...")
    print("="*60)
    
    import os
    
    def greet(name: str) -> str:
        """å‘æŸäººæ‰“æ‹›å‘¼"""
        return f"ä½ å¥½ï¼Œ{name}ï¼å¾ˆé«˜å…´è§åˆ°ä½ ã€‚"
    
    try:
        # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä½¿ç”¨ Ollama
        os.environ["OPENAI_API_KEY"] = "ollama"
        os.environ["OPENAI_BASE_URL"] = "http://localhost:11434/v1"
        
        # åˆ›å»ºæµ‹è¯• Agent
        agent = Agent(
            name="test_agent",
            system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ï¼Œæ“…é•¿ä½¿ç”¨å·¥å…·ã€‚å½“éœ€è¦æ‰“æ‹›å‘¼æ—¶ï¼Œä½¿ç”¨ greet å·¥å…·ã€‚",
            tools=[greet],
            model=model_name,  # ä½¿ç”¨å®é™…å®‰è£…çš„æ¨¡å‹
            max_iterations=3
        )
        
        # æµ‹è¯•æŸ¥è¯¢
        test_query = "è¯·å‘ 'å°æ˜' æ‰“ä¸ªæ‹›å‘¼"
        print(f"ğŸ‘¤ æµ‹è¯•é—®é¢˜: {test_query}")
        print("â³ ç­‰å¾…å“åº”...\n")
        
        response = agent.input(test_query)
        
        print(f"ğŸ¤– Agent å›å¤:")
        print(f"{response}")
        print("\nâœ… æµ‹è¯•æˆåŠŸï¼Ollama é›†æˆæ­£å¸¸å·¥ä½œã€‚")
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        print("\nğŸ’¡ æ’æŸ¥å»ºè®®:")
        print("1. ç¡®è®¤ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ: ollama serve")
        print("2. ç¡®è®¤å·²ä¸‹è½½æ¨¡å‹: ollama run llama3.2")
        print("3. æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
        print("4. å°è¯•é‡å¯ Ollama æœåŠ¡")
        return False


def print_recommendations():
    """æ‰“å°ä½¿ç”¨å»ºè®®"""
    print("\n" + "="*60)
    print("ğŸ“š æ¨èçš„ Ollama æ¨¡å‹")
    print("="*60)
    print("""
ğŸ‡¨ğŸ‡³ ä¸­æ–‡å‹å¥½ï¼š
  ollama run qwen2.5:7b        # åƒé—® 7Bï¼ˆæ¨èä¸­æ–‡ä½¿ç”¨ï¼‰
  ollama run chatglm3          # ChatGLM3

âš¡ å¿«é€Ÿè½»é‡ï¼š
  ollama run llama3.2          # Llama 3.2 1B
  ollama run llama3.2:3b       # Llama 3.2 3Bï¼ˆå¹³è¡¡ï¼‰

ğŸ¯ é«˜è´¨é‡ï¼š
  ollama run mistral:7b        # Mistral 7B
  ollama run deepseek-r1:7b    # DeepSeek R1

ğŸ“Š ä½¿ç”¨å»ºè®®ï¼š
  - å¼€å‘æµ‹è¯•: llama3.2 æˆ– llama3.2:3b
  - ä¸­æ–‡ä»»åŠ¡: qwen2.5:7b
  - ç”Ÿäº§ç¯å¢ƒ: æ ¹æ®ç¡¬ä»¶é€‰æ‹©æ›´å¤§æ¨¡å‹
    """)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¦™ Ollama + ConnectOnion é›†æˆæµ‹è¯•")
    print("="*60)
    
    # æ£€æŸ¥æœåŠ¡
    service_ok, models = check_ollama_service()
    
    if not service_ok:
        print("\nâŒ Ollama æœåŠ¡æœªè¿è¡Œæˆ–é…ç½®ä¸æ­£ç¡®")
        print("\nğŸ“ å¿«é€Ÿå¼€å§‹æŒ‡å—:")
        print("1. å®‰è£… Ollama:")
        print("   macOS/Linux: curl -fsSL https://ollama.com/install.sh | sh")
        print("   Windows: è®¿é—® https://ollama.com/download")
        print("\n2. ä¸‹è½½æ¨¡å‹:")
        print("   ollama run llama3.2")
        print("\n3. å¯åŠ¨æœåŠ¡:")
        print("   ollama serve")
        print("\n4. é‡æ–°è¿è¡Œæ­¤è„šæœ¬:")
        print("   python test_ollama.py")
        return
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨æ¨¡å‹
    if not models:
        print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°å·²å®‰è£…çš„æ¨¡å‹")
        print("ğŸ’¡ è¯·å…ˆä¸‹è½½ä¸€ä¸ªæ¨¡å‹: ollama run llama3.2")
        return
    
    # é€‰æ‹©æµ‹è¯•æ¨¡å‹
    test_model = None
    
    # ä¼˜å…ˆä½¿ç”¨ llama3.2
    for model in models:
        model_name = model.get("name", "")
        if "llama3.2" in model_name:
            test_model = model_name
            break
    
    # å¦‚æœæ²¡æœ‰ llama3.2ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
    if not test_model and models:
        test_model = models[0].get("name")
        print(f"\nâš ï¸  æœªæ‰¾åˆ° llama3.2 æ¨¡å‹")
        print(f"ğŸ’¡ å½“å‰å·²å®‰è£…: {[m.get('name') for m in models]}")
        print(f"ğŸ’¡ å°†ä½¿ç”¨ {test_model} è¿›è¡Œæµ‹è¯•")
        print(f"ğŸ’¡ å»ºè®®ä¸‹è½½: ollama run llama3.2")
        print()
    
    if not test_model:
        print("\nâŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
        return
    
    # è¿è¡Œæµ‹è¯•
    success = test_basic_tool(test_model)
    
    if success:
        print_recommendations()
        print("\nğŸ‰ æ­å–œï¼ä½ å¯ä»¥å¼€å§‹ä½¿ç”¨ Ollama äº†ï¼")
        print("\nğŸ“ ä¸‹ä¸€æ­¥:")
        print("1. è¿è¡Œä½ çš„ Agent: python agent.py")
        print("2. åœ¨ agent.py ä¸­è®¾ç½®: USE_OLLAMA = True")
        print("3. æŸ¥çœ‹å®Œæ•´æ–‡æ¡£: cat OLLAMA-SETUP.md")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æµ‹è¯•ä¸­æ–­")
        sys.exit(0)

